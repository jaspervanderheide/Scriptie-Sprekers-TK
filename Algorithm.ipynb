{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\m6800\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "c:\\users\\m6800\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sn\n",
    "from sklearn import datasets\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.sklearn_api.w2vmodel import W2VTransformer\n",
    "from gensim.sklearn_api.d2vmodel import D2VTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from ast import literal_eval\n",
    "import xlrd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'f1-scores': scores,\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Handelingen.csv\", index_col=0)\n",
    "df = df.loc[df['speech category'] == 'Main Speech']\n",
    "stemmer = SnowballStemmer(\"dutch\")\n",
    "\n",
    "## FIX THIS INTO ONE LINE\n",
    "list_text = df.text.apply(lambda x: [stemmer.stem(t) for t in tokenizer.tokenize(x)])\n",
    "normal_text = df.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Models.xlsx', index_col=0)\n",
    "df3 = df.loc[df.DOEN == True]\n",
    "models = {}\n",
    "\n",
    "for x,y in zip(df3.Classifier,df3.PIPELINE):\n",
    "    exec(compile(\"a=\"+y,'','exec'))\n",
    "    models[x] = Pipeline(a)    \n",
    "params = {}\n",
    "\n",
    "for clf in set(df3.Classifier):\n",
    "    test = df3.loc[df3.Classifier == clf]\n",
    "    test = test.loc[test.DOEN == True]\n",
    "    test = test.dropna(axis=1, how='all')\n",
    "    test= test.drop([col for col in test.columns if \"__\" not in col], axis =1)\n",
    "    test = test.to_dict(orient='list')\n",
    "    testdict = {}\n",
    "    for x,y in test.items():\n",
    "        a=literal_eval(y[0])\n",
    "        #exec(compile(\"a=\"+y[0],'','exec'))\n",
    "        testdict[x] = a   \n",
    "    params[clf] = testdict\n",
    "rawdict = {x:y for x,y in zip(df3.Classifier,df3.RAW)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,pipe in models.items():\n",
    "    if rawdict[name]:\n",
    "         text = list(list_text)\n",
    "    else:\n",
    "        text = list(normal_text)\n",
    "    gs = GridSearchCV(pipe, params[name], cv=5, scoring='f1_weighted')\n",
    "    gs.fit(text, df.party)\n",
    "    rows = [row(name, gsc.cv_validation_scores, gsc.parameters) for gsc in gs.grid_scores_]\n",
    "    df2 = pd.concat(rows, axis=1).T\n",
    "    pd.concat([df2,pd.read_csv('Scores.csv', index_col=0)], ignore_index=True).sort_values(['mean_score'], ascending=False).to_csv('Scores.csv')\n",
    "    df.loc[df.Classifier==name,\"DOEN\"]=False\n",
    "    df.to_excel('Models.xlsx')\n",
    "    \n",
    "#https://stackoverflow.com/questions/46735847/save-best-params-in-gridsearch-in-a-pandas-dataframe\n",
    "# https://stackoverflow.com/questions/36271413/pandas-merge-nearly-duplicate-rows-based-on-column-value?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
