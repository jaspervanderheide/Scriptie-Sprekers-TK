\section{Methodologie}
\label{sec:meth}


\subsection{De data}
\label{data}
De data die gebruikt worden, zijn de Handelingen van de Tweede Kamer gedurende het missionaire kabinet-Rutte II (5 november 2012 tot 22 maart 2017). Er is gekozen voor dit kabinet, omdat de data hiervoor makkelijk verkrijgbaar was, het kabinet lang zat - waardoor er veel data is - en het recent is waardoor het makkelijker te interpreteren is. In dit kabinet zaten de PvdA en VVD. Deze data zijn in xml-formaat van de website officielebekendmakingen.nl gehaald samen met bijbehorende metadatabestanden. De bestanden van de Handelingen bevatten voornamelijk informatie over spreekbeurten tijdens een debat, waaronder naam van een spreker, partij-affiliatie, inhoud van de spreekbeurt en het soort spreekbeurt. Deze gegevens zijn samengevoegd tot één tabel.\par
Deze dataset bestaat uit een aantal soorten spreekbeurten; debatbijdragen, interrupties en antwoorden. Een debatbijdrage is de eerste onafgebroken spreekbeurt die een spreker geeft achter een spreekgestoelte, aangeduid in de xml-file met het attribuut \textit{nieuw="ja"}. Dit kan een bijdrage in een debat zijn of een vraag tijdens een vragenuur. Interrupties zijn de vragen die andere politici stellen vanachter de interruptiemicrofoon aan een spreker. De antwoorden zijn vervolgens de reactie van een spreker achter het spreekgestoelte op een interruptie. Aangezien een debatbijdrage geïnterrumpeerd kan worden, kan deze inhoudelijk doorlopen in een antwoord van een spreker.  Gerelateerde onderzoeken voegen vaak alle teksten van een spreker samen tot één document. Dit is alleen niet mogelijk met de hoeveelheid kleine partijen in de Tweede Kamer, die dan niet altijd in een training of test set zijn vertegenwoordigd. Daarom is in dit onderzoek ervoor gekozen om een debatbijdrage met alle bijbehorende antwoorden samen te voegen tot één document voor de classificatie.\par
Daarnaast zijn er verschillende soorten sprekers; de voorzitter, Tweede Kamerleden, leden van het kabinet en gastsprekers. Hieruit is alleen gekozen voor sprekers waarvan er een partij-affiliatie vermeld staat. Dit is niet het geval voor leden van het kabinet, de voorzitter en gastsprekers met uitzondering van Nederlandse leden van het Europees Parlement.\par
Deze dataset bevat vervolgens naast de verkozen partijen na de Tweede Kamerverkiezingen van 2012 ook afsplitsingen van die partijen (tien in totaal) en bezoeken van vertegenwoordigingen van Nederlandse partijen uit het Europees Parlement (tien in totaal). Omdat van beide categorieën relatief weinig data is en er overlap zit met hun oorspronkelijke of gelieerde partij, zijn deze er uit gehaald. 50PLUS is in 2014 \cite{50PLUSNRC} uiteengevallen in twee fracties die aanspraak maakten op de partij-affiliatie 50PLUS. Vanaf dit moment zijn deze documenten niet meer meegenomen om onduidelijkheid te voorkomen.\par

De documenten verschillen in grootte. De distributie van documentgrootte lijkt op een lognormale verdeling, maar met een Kolmogorov-Smirnov test is hier geen bewijs voor gevonden \cite{Scipy}.

\begin{figure}[H]
    \centering
    \hspace*{-1in}
    \subfloat[Normale x-as]{{\includegraphics[width=9cm]{Verslag/Tables/lengthtexts.png} }}%
    \subfloat[Logaritmische x-as]{{\includegraphics[width=9cm]{Verslag/Tables/lengthtextslog.png} }}%
    \caption{Aantal woorden per document}%
    \label{fig:example}%
\end{figure}
Om toch de uitschieters er uit te halen, is aangenomen dat de distributie wel lognormaal verdeeld is en zijn daarmee de documenten buiten het betrouwbaarheidsinterval van 95\% eruit gehaald. De documenten met een lengte van minimaal 28 en maximaal 1492 woorden bleven daarmee over. De gemiddelde documentlengte is daarna 498 woorden en de mediaan is 386 woorden.\par

\begin{table}[H]
\label{aantallen}
\caption{Aantal documenten per partij gedurende het missionaire kabinet-Rutte II.}
\centering
\input{Handmatig/Spreekbeurten.tex}
\end{table}
Deze 14899 documenten zijn verdeeld over 2984 debatten, waarbij elke vraag tijdens het vragenuur als één debat gezien wordt. Op basis van de aantallen is er voor classificatie een baseline \textit{accuracy} van 0.15 (door altijd grootste partij te kiezen) en baseline $F_1$ score van 0.11 (door te gokken gewogen bij aantal documenten van een partij).\par


\subsection{Methoden}


\subsubsection{DV1: Beste classificatiemethode}
Om deze deelvraag te beantwoorden zullen een aantal classificatiemethoden vergeleken worden. Aangezien het niet mogelijk is om alle classificatiemethoden te vergelijken, beperkt dit onderzoek zich tot classificatiemethoden die gebruikt zijn in vergelijkbare onderzoeken, zoals besproken in sectie \ref{sec:Deelvraag1}. Er is ervoor gekozen om alleen gebruik te maken van methoden waarvan reeds implementaties beschikbaar waren in scikit-learn. Voor alle methoden wordt gezocht naar de beste parameters, ook wel bekend als een grid search. Deze grid search wordt gedaan door 5-fold cross-validation, waarbij de training set steeds 80\% is en de test set 20\% van de totale dataset. Een totaal aantal van 6480 combinaties van methoden en parameters zijn getest. De verwachting is dat de scores lager zijn dan die gevonden in het gerelateerd werk, omdat de documentgrootte kleiner is en de baseline scores lager.

\paragraph{Pre-processing}
Voor pre-processing is gebruik gemaakt van tokenisation en lowercasing. Voor tokenisation is de reguliere expressie $\\w+$ gebruikt, waardoor alles behalve letters en cijfers weggehaald wordt. Vervolgens is er gevarieerd tussen wel of geen gebruik maken van stemming. In het geval van stemming is gebruik gemaakt van de Snowball Stemmer van de Python NLTK module.

\paragraph{Bag-of-words model}
Bag-of-words model is de meest gebruikte representatie van data in vergelijkbare onderzoeken. Bij het bag-of-words model wordt elk document gerepresenteerd als een vector, waarbij elke kolom een woord is met een bijbehorende waarde. Voornaamste beperking van dit model is dat het geen rekening houdt met de volgorde van woorden, wat een groot effect kan hebben op de betekenis van een document.\par
Voor dit onderzoek zijn de volgende wegingen voor woorden getest: \textit{boolean} (wel of niet aanwezig), \textit{tf} (woordfrequentie), \textit{tf-norm} (woordfrequentie genormaliseerd door documentlengte) en \textit{tf-idf} (woordfrequentie gecompenseerd voor documentfrequentie). Daarnaast wordt in dit onderzoek geëxperimenteerd met een minimale of maximale woord- of documentfrequentie. Ook is gekeken naar het effect van combinaties van de volgende n-grams; unigrams, bigrams en trigrams. N-grams zijn combinaties van N aantal opeenvolgende woorden. Bij een unigram is elke feature gewoon één woord, terwijl bij een bigram dit twee opvolgende woorden zijn. Dit kan nuttig zijn, want als bijvoorbeeld het woord \textit{asfalt} er in voorkomt, dan is het voor ideologie mogelijk relevant of er \textit{minder asfalt} of \textit{meer asfalt} staat.\par

\paragraph{Support Vector Machines en Logistische Regressie}
De meest voorkomende techniek in vergelijkbaar onderzoek is Support Vector Machines (SVM). Een andere techniek die gebruikt wordt is logistische regressie. Beide kennen een eigen implementatie in scikit-learn, maar deze implementaties zijn niet efficiënt met grote datasets. Om deze reden is er in beide gevallen voor gekozen om gebruik te maken van de functie SGDClassifier, die beide technieken leert met \textit{stochastic gradient descent learning}. Voor regularisatie is hier geëxperimenteerd met L1 en L2 regularisatie, en een combinatie van beide genaamd Elasticnet. De andere parameters zijn gelaten op de standaardwaarden van scikit-learn \cite{scikit-learn}. Een belangrijke onaangepaste waarde is die van maximaal aantal iteraties, waarvoor de scikit-learn standaard 5 is. Volgens scikit-learn convergeert de SGDClassifier rond de $10^{6}/n$ iteraties waar $n$ het aantal documenten in de training set is. In het geval van deze dataset zou dat 84 iteraties zijn. Vanwege de grootte van de grid search was het voor dit onderzoek niet mogelijk het maximaal aantal iteraties te verhogen tijdens de grid search. De resultaten buiten de grid search zullen gebaseerd zijn op een maximaal aantal iteraties van 100.\par
% https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4

\paragraph{Naive Bayes}
Een andere techniek die gebruikt wordt voor politieke tekstclassificatie is Naive Bayes. Dit algoritme neemt aan dat elke \textit{feature} onafhankelijk is ten op zichte van de rest. Dit is bij tekstclassificatie vaak niet het geval omdat het gebruik van sommige woorden gepaard kan gaan met het gebruik van andere woorden. Daarnaast is het gebruik van meerdere n-grams in een classificatie schending van de aanname, want als bijvoorbeeld een bigram er in voorkomt dan komen ook beide unigrams er in voor. Desalniettemin blijkt Naive Bayes effectief te zijn voor tekstclassificatie \cite{bhand,scikit-learn}. Hiervoor zijn de functies van scikit-learn MultinomialNB en BernoulliNB gebruikt \cite{bhand,scikit-learn}.\par

\paragraph{Beoordelen van kwaliteit}
De meest gebruikte methoden om kwaliteit van politieke tekstclassificatie te beoordelen zijn \textit{accuracy} en $F_1$ score, die opgebouwd is uit \textit{recall} en \textit{precision}. Deze scores worden berekend op basis van vier variabelen. Deze variabelen geven weer hoeveel documenten wel of niet bij een partij horen, en of deze wel of niet als dusdanig zijn geclassificeerd \cite{Manning:2008:IIR:1394399} .\par

\begin{table}[H]
\label{tab:scores}
\centering
\begin{tabular}{l|l|l}
 & Behorend tot partij & Niet behorend tot partij\\ \hline
Geclassificeerd als partij &   \textit{true positive (tp)} & \textit{false positive (fp)} \\ \hline
Niet geclassificeerd als partij & \textit{false negative (fn)} & \textit{true negative (tn)} \\
\end{tabular}
\end{table}
\begin{equation}
    Precision = \frac{tp}{tp + fp}\\
\end{equation}
\begin{equation}
    Recall = \frac{tp}{tp + tn}
\end{equation}
\begin{equation}
    Accuracy = \frac{tp + tn}{tp + tn + fp + fn}
\end{equation}
\begin{equation}
    F_1 = 2 * \frac{Precision * Recall}{Precision + Recall}
\end{equation}
\textit{Accuracy} is het percentage van documenten dat correct geclassificeerd is. \textit{Accuracy} wordt voor de hele classificatie gedaan en niet per klasse. \textit{Precision} is het percentage van documenten geclassificeerd als een partij, dat ook bij die partij hoort. \textit{Recall} is het percentage documenten van documenten behorende tot een partij, dat ook als die partij geclassificeerd is. $F_1$ is het harmonisch gemiddelde van \textit{recall} en \textit{precision}. \textit{Precision}, \textit{recall} en daarmee $F_1$ worden per partij berekend. Er zijn drie varianten om deze scores voor de hele classificatie te berekenen. \par
Allereerst is er \textit{micro}, waarbij alle variabelen bij elkaar opgeteld worden en vervolgens de scores berekend. Dit leidt ertoe dat resultaten van partijen met veel documenten belangrijker zijn. Als een classificatie kleine partijen grotendeels fout classificeert, kan deze score alsnog hoog zijn. In het geval van meer dan twee partijen is dit hetzelfde als \textit{accuracy}.\par 
Als tweede is er \textit{macro}, waarbij alle scores per partij berekend worden en daarvan het gemiddelde wordt genomen. Dit leidt er dan weer toe dat resultaten van partijen met weinig documenten net zo belangrijk zijn. Hierdoor kan een classificatie met een laag aantal correct geclassificeerde documenten hoog scoren door vooral kleine partijen goed te classificeren.\par
Als laatste is er dan nog \textit{gewogen}, deze berekent net als \textit{macro} de scores per partij, maar neemt hiervan het gemiddelde gewogen bij het aantal documenten behorend tot een partij. Deze wijkt weinig af van de $micro$ variant, tenzij er uitschieters zijn bij partijen.\par
Aangezien $micro$ al terugkomt in \textit{accuracy} en het nadeel van \textit{macro} te groot is omdat de partijen nogal variëren in grootte, is gekozen voor \textit{gewogen} $F_1$ score naast \textit{accuracy}.
\bigskip

\subsubsection{DV2: Invloed van namen}
In Diermeier et al. \cite{diermeier_godbout_yu_kaufmann_2012} wordt aangenomen dat namen een groot effect hebben op de classificatie en Hirst et al. \cite{Hirst_textto} bevestigen dit voor het Europees Parlement. Aangezien hier bij deelvraag 1 niet voor is gekozen, wordt bij deze deelvraag gekeken hoe groot het effect hiervan is, specifiek gericht op partijnamen en achternamen van Kamerleden. \par
Voor deze deelvraag wordt wederom een classificatie gedaan met de classificatiemethode die resulteerde uit deelvraag 1. In deze classificatie worden alle partijnamen vervangen door \textit{PARTIJNAAM} en alle achternamen van Kamerleden vervangen door \textit{KAMERLIDNAAM}. Deze namen zijn uit de Handelingen gehaald. Voor partijnamen zijn ook lidwoorden toegevoegd, voor achternamen van Kamerleden zijn ook verkortingen meegenomen. Dit laatste omdat bijvoorbeeld \textit{Van Haersma Buma} vaak aangesproken wordt als \textit{Buma}. Voornamen van Kamerleden worden zelden tot nooit gebruikt, dus die zijn er niet uitgehaald. Een nadeel van deze aanpak is dat ook namen van niet-Kamerleden of andere woorden weggehaald kunnen worden als deze hetzelfde zijn als naam van een Kamerlid. Door gebruik van gevoeligheid voor hoofdletters is geprobeerd dit te voorkomen. Een opvallend voorbeeld hiervan is de naam Rutte, die zowel behoort tot het Kamerlid Arno Rutte als de premier Mark Rutte. Steekproefgewijs is gekeken of er nog namen achter zijn gebleven, maar die zijn niet gevonden. \par
Ook wordt gekeken naar classificatie met alleen partijnamen en achternamen van Kamerleden. Alle andere woorden worden weggehaald. Namen van Kamerleden en partijen die niet aan elkaar geschreven worden, zoals \textit{Partij van de Arbeid}, worden aan elkaar geschreven zodat het één feature wordt. Doordat alle andere woorden weggehaald zijn, worden de bi- en trigrams combinaties van namen die zinnen uit elkaar kunnen staan, dus die niet meer informatie geven dan unigrams. Daarom wordt er gebruikt van de classificatiemethode uit deelvraag 1, maar dan met alleen unigrams. Hoge scores voor deze classificatie geven aan dat met alleen namen classificatie goed te doen is en dat dit dus een grote bijdrage heeft geleverd aan de resultaten uit deelvraag 1. \par
Op basis van vergelijkbaar onderzoek is de hypothese dat de achternamen van Kamerleden en partijnamen van invloed zijn. Hiervoor verwachten we dat voor de classificatie zonder namen de scores een stuk lager zijn dan deelvraag 1 en de scores van de classificatie met alleen namen aanzienlijk hoger zijn dan de baseline scores.

\subsubsection{DV3: Oppositie of regering}

Om deze deelvraag te beantwoorden zijn drie experimenten uitgevoerd. Twee daarvan zijn gebaseerd op experimenten uit Hirst et al. \cite{Hirst_textto} voor dezelfde vraag. De derde is ontwikkeld voor dit onderzoek. Met deze laatste wordt begonnen. Bij deze deelvraag is de classificatiemethode uit deelvraag 1 zonder achternamen van Kamerleden en partijnamen gebruikt. De hypothese is op basis van de bevindingen van Hirst et al. dat de classificatie inderdaad afhankelijk is van partij-status.\par
Als er een afhankelijkheid is van partij-status, dan is te verwachten dat het aantal misclassificaties minus verwachte waarde binnen regeringspartijen en binnen oppositiepartijen hoger ligt dan tussen oppositiepartijen en regeringspartijen. De verwachte waarde is afhankelijk van het aantal documenten van een partij in de training set \cite{Sahare}. Aangezien de test set uit dezelfde set als de training is gehaald, is de verwachte waarde ook afhankelijk van het aantal documenten van een partij in de test set. Uit de voorverkenning (op basis van resultaten uit deelvraag 1 en 2) blijkt deze correlatie tussen het aantal \textit{false positives} van een partij en het aantal documenten behorend tot die partij.
\begin{figure}[H]
  \centering
    \includegraphics[width=0.60\paperwidth]{Verslag/Handmatig/Correlation.png}
\caption{Het aantal \textit{false positives} ten opzichte van het aantal documenten behorend tot die partij (\textit{false negatives} en \textit{true positives}). Dit is op basis van 100 classificaties met verschillende test en train set. De Pearson correlatie is 0.77 en de p-waarde \num{5.40e-101}.}
\label{fig:correlation}
\end{figure}
Op basis van dit verband definiëren we het verwachte aantal documenten van partij $i$ die foutief geclassificeerd worden als partij $j$ als
\begin{equation}
\label{eq:expected}
V_{i,j}  = fn_i *  \frac{tp_{j}+fn_{j}}{tn_{i}+fp_{i}}
\end{equation}
waar $i\neq j$. De teller van de breuk is het aantal documenten die bij partij $j$ horen en de noemer het aantal documenten die niet bij partij $i$ horen. Op deze manier is $\sum_{j=0}^{n} (V_{i,j}) = fn_i$ waar $n$ het aantal partijen is minus partij $i$.\par
De error ($e_{i,j}$) is dan het verschil van  het daadwerkelijk aantal misclassificaties ($D_{i,j}$) en de verwachte waarde ($V_{i,j}$)
\begin{equation}
\label{eq:error}
e_{i,j} = D_{i,j} - V_{i,j}
\end{equation}
met opnieuw $i\neq j$ en $i$ de echte partij waar een document bijhoort en $j$ de voorspelde partij. \par
Als dit een goede benadering is van de error, dan is het te verwachten dat deze normaal verdeeld is \cite{citeulike:7531484}. Om te kijken of er een bias is, worden de distributies binnen regeringspartijen en binnen oppositiepartijen vergeleken met de distributie tussen beide groepen. Om de invloed van variantie door de willekeurige splitsing documenten voor trainen en testen te beperken, wordt de classificatie 100 keer gedaan. In het geval dat de distributies normaal verdeeld zijn, zal de statistische test plaatsvinden op basis van een eenzijdige t-toets. Als de distributies niet normaal verdeeld zijn, zal dit plaatsvinden door een Mann-whitneytoets. Het gekozen significantieniveau ($\alpha$) is 0.05. De nulhypothese is dat er geen verschil is tussen de verdelingen. De alternatieve hypothese is dan dat de distributie van binnen oppositie of regering groter is dan die tussen een regerings- en oppositiepartij. Op basis van de bevindingen van Hirst et al. is de hypothese van dit onderzoek dat de nulhypothese verworpen kan worden.\par
In het eerste experiment uit Hirst et al. zullen de meest karakteristieke woorden per partij van de ene zittingsperiode vergeleken worden met de meest karakteristieke woorden per partij van de andere zittingsperiode. Als de classificatie op basis van ideologie is in plaats van partij-status, is het te verwachten dat de woorden bij een partij blijven en niet gekoppeld zijn aan in oppositie of regering zitten. Op basis van de hypothese is de verwachting dat de nulhypothese verworpen kan worden en de alternatieve hypothese aangenomen kan worden.\par
In het tweede experiment uit Hirst et al. worden classificaties getraind op een zittingsperiode en getest op een andere zittingsperiode, waar een kabinet uit andere partijen bestond. Als de classificatie afhankelijk is van partij-status is de verwachting dat de scores van partijen die gewisseld zijn van partij-status sterker dalen dan partijen die niet van partij-status zijn veranderd. Op basis van de hypothese is dan ook de verwachting dat bij de partijen die gewisseld zijn partij-status een sterkere daling te zien is.\par
Als vergelijkingsmateriaal is voor deze experimenten een tweede dataset nodig uit een ander kabinet. Hiervoor is het wenselijk dat dit kabinet bestaat uit andere partijen dan kabinet-Rutte II. Daarnaast is het ook wenselijk als het niet te ver terug is, zodat onderwerpen en taalgebruik enigszins overeenkomstig zijn. Omdat kabinet-Rutte I een minderheidskabinet was met een bijzondere partij-status voor de PVV, is ervoor gekozen om de Handelingen van de Tweede Kamer tijdens het missionaire kabinet-Balkenende IV (22 februari 2007 tot 20 februari 2010) te gebruiken. Dit kabinet bestond uit CDA, PvdA en ChristenUnie.\par
De partij 50PLUS bestond nog niet gedurende kabinet-Balkenende IV, dus documenten van deze partij zijn weggelaten. Verder heeft dezelfde verwerking van data plaatsgevonden, zoals beschreven in \ref{data}. Alleen de minimum- en maximumlengte is overgenomen van de dataset van kabinet-Rutte II.\par

\begin{table}[H]
\label{aantallenBal}
\caption{Aantal documenten per partij gedurende het missionaire kabinet-Balkenende IV.}
\centering
\input{Handmatig/SpreekbeurtenBal.tex}
\end{table}

\subsubsection{DV4: Links-rechts as}
% HYPOTHESE EN ANDERE SHIT
Als de classificatie afhankelijk is van positie op de links-rechts as dan is het te verwachten dat, net als bij deelvraag 3, de misclassificaties minus de verwachte waarde groter zijn als twee partijen dichterbij elkaar staan op de links-rechts as.  Daarvoor zal wederom formule \ref{eq:expected} gebruikt worden als verwachte waarde en formule \ref{eq:error} als error.\par
Er zijn verschillende methoden om partijen in te delen op een links-rechts as. Er is hier gekozen voor de indeling van het Manifesto Project \cite{Volkens:2017}. Het Manifesto Project geeft scores op een heel aantal politieke posities, waaronder dus de links-rechts as, op basis van het verkiezingsprogramma van dat jaar, in dit geval dus van 2012.\par
\begin{table}[H]
\centering
\caption{Scores op de links-rechts as per partij van het Manifesto Project voor de verkiezingsprogramma's van 2012.}
\label{my-label}
\centering
\begin{tabular}{lc}
\toprule
Partij  & Score van Manifesto Project \\
\midrule
SP           & -20.926 \\ 
GroenLinks   & -9.584 \\ 
PvdA         & -6.558 \\ 
PvdD         & -6.465 \\ 
50PLUS       & -6.311\\ 
D66          & -0.778\\ 
ChristenUnie & 10.203\\ 
PVV          & 15.642\\ 
CDA          & 17.701\\
VVD          & 22.629\\ 
SGP          & 26.6\\ 
\bottomrule
\end{tabular}
\end{table}
Er wordt vervolgens gekeken door middel van een Pearson correlatie toets of er een correlatie is tussen de error van twee partijen en de afstand op de links-rechts as van die partij. Het significantieniveau ($\alpha$) hiervoor is opnieuw 0.01. De nulhypothese is dat er geen negatieve correlatie is tussen de error en de afstand op de links-rechts as. De alternatieve hypothese is dat er wel een negatieve correlatie is tussen de error en de afstand op de links-rechts as.\par
Als uit deelvraag 3 blijkt dat partij-status invloed heeft op de error, zal bovenstaande methode ook uitgevoerd worden voor de aparte combinaties; binnen oppositie en tussen regeringspartij en oppositiepartij. Binnen regering is dit niet mogelijk aangezien er maar één afstand is (tussen PvdA en VVD).\par

\subsubsection{DV5: Woordgebruik van sprekers}
De vorige classificaties trainden op documenten en werden getest op andere documenten, maar wel van dezelfde sprekers als uit de training set. Naast de ideologie kan de classificatie daarom ook getraind zijn op het taalgebruik van sprekers. Als een Kamerlid bijvoorbeeld een woord regelmatig in speeches gebruikt, maar niet wordt gebruikt door zijn partijgenoten, wordt dit wel gezien als een belangrijk woord voor de classificatie naar partij-affiliatie. Hirst et al. \cite{Hirst_textto} plaatsten al een soortgelijke kanttekening bij de resultaten van Diermeier et al. \cite{diermeier_godbout_yu_kaufmann_2012}. De hypothese is dat de classificatie afhankelijk is van woordgebruik van sprekers\par
Om te kijken of dit effect er is, wordt er opnieuw een classificatie gedaan met de classificatiemethode uit deelvraag 1 zonder achternamen van Kamerleden en partijnamen. Ditmaal worden alleen niet de individuele documenten verdeeld over de training en test set, maar worden de Kamerleden, met bijbehorende documenten, verdeeld over de training en test set. Als taalgebruik van een spreker in de training set voorheen invloed had op de classificatie, zal dat nu geen effect meer hebben omdat er geen documenten van die spreker meer voorkomen in de test set. De verwachting is daarom ook dat deze classificatie lagere scores vindt dan die van deelvraag 2.\par

% hypothese

% https://www.google.nl/search?q=grafiek+2012+kieskompas&safe=off&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjigpaDoIXbAhUSJlAKHUBzBQ4Q_AUoAXoECAAQAw&biw=1920&bih=943#imgrc=Dekv0sSQBTnikM: