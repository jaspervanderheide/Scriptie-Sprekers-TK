\section{Gerelateerd werk}
\label{sec:rel}

Toespraken in parlementen worden veel gebruikt in tekstclassificatie, omdat deze veel nette tekst bevatten en vaak gelabeld zijn. Labels zijn bijvoorbeeld naam en partij van de spreker, maar ook daar uit afleidbare labels zoals geslacht, leeftijd en partij-status (oppositie of regering).\par
In dit hoofdstuk zullen verschillende onderzoeken behandeld worden die tekstclassificatie hebben toegepast op parlementaire teksten. Eerst zullen de onderzoeken algemeen besproken worden. Vervolgens zal uitgebreider gekeken worden naar de effecten van verschillende classificatiemethoden. In de latere secties zullen aspecten besproken worden die in vergelijkbare onderzoeken genoemd worden als van invloed op de classifcatie.

\subsection{Tekstclassificatie van parlementaire teksten}

Diermeier et al. deden onderzoek naar het classificeren op basis van ideologische positie in de Amerikaanse Senaat\cite{diermeier_godbout_yu_kaufmann_2012}. Ze trainden hun classificatie op de speeches van de 25 meest liberale en de 25 meest conservatieve senatoren van het 101e tot en met het 107e Congres en testten op dezelfde categorieën van het 108e Congres. Een document was in dit onderzoek de verzameling van alle speeches van een senator in een congres. Deze classificatie resulteerde uiteindelijk in een \textit{accuracy} van 94\% (baseline van 50\%). Van de 50 senatoren in de test set, kwamen er 44 al voor in de training set, doordat de training op voorgaande congressen was.\par
Later in het onderzoek vergeleken ze ook de 25 gematigd conservatieve  en de 25 gematigd liberale senatoren van dezelfde congressen. Het resultaat hiervan was 52\% (baseline van 50\%), dus nauwelijks beter dan gokken. Als verklaring voor dit verschil ten opzichte van de uitersten zeggen ze dat gematigden een minder duidelijke ideologie hebben.\par
Yu et al. \cite{doi:10.1080/19331680802149608} richtte zich vervolgens op zowel het Amerikaanse Huis van Afgevaardigden als de Senaat in 2005. Een document was in dit onderzoek de verzameling van alle speeches van een senator in een Congres en het label de partij. Voor het Huis van Afgevaardigden vonden ze een \textit{accuracy} van 80.1\% (baseline van 51.5\%) en voor de senaat 86.0 \% (baseline van 55.0\%). Ze testten hun classificaties ook op de andere kamer. Van Huis van Afgevaardigden naar senaat leverde dit een \textit{accuracy} op van 88.0\% (baseline van 55.0\%) en andersom 67.6\% (baseline van 51.5\%). Hun verklaring voor dit verschil is dat het Huis van Afgevaardigden sterker verdeeld is langs partijlijnen.\par
Vervolgens herhaalden ze de classificaties op het Huis van Afgevaardigden uit 2015, maar testten ditmaal op de Senaat elk jaar tussen 1989 en 2006 afzonderlijk. Hier zien zij een stijging in \textit{accuracy} van 60\% (baseline van 55.0\%) in 1989 naar 87.0\% (baseline van 55.0\%) in 2006, maar met twee duidelijke dalen. Ze presenteren twee mogelijke verklaringen voor de trend; het veranderen van de onderwerpen en het sterker verdeeld worden van het congres. \par
Als een vervolg op deze onderzoeken deden Hirst et al. een vergelijkbaar onderzoek naar het Canadese Parlement \cite{Hirst_textto}. Hierbij werd zowel gekeken naar de Engelse als Franse teksten. Een document werd hier gezien als de samenvoeging van alle spreekbeurten van een spreker. Afhankelijk van taal en dataset vinden zij in dit onderzoek \textit{accuracy} scores van 83.2\%  en hoger (baseline van 65.5\%).\par
Het onderzoek bevat ook een classificatie van het Europees Parlement. Hierbij voegen ze alle teksten van een parlementslid bij elkaar en delen die op in stukken van gelijke grootte. Zij vinden voor documentgrootte van 267 woorden een \textit{accuracy} van 44.0\% oplopend tot 61.8\% (baseline van 38-39\%) voor documentgrootte van 6666.\par

Het onderzoek van Bhand et al. richtte zich op het classificeren van leden van het Amerikaanse congres in 2005, op basis van affiliatie (Republikeins of Democratisch)\cite{bhand}. Een document hierbij was in tegenstelling tot eerdergenoemde onderzoeken een speech. Zij vonden hiervoor uiteindelijk een $F_1$ score van 0.68 (baseline niet vermeld).\par

Ferreira probeerde interventies van politici te classificeren op basis van geslacht, leeftijdsgroep, partij-affiliatie en ori\"{e}ntatie in het Portugese parlement \cite{Ferreira2016UsingTT}. In het geval van classificatie op basis van partij-affiliatie bereikte men een $F_1$ score van 0.90 (baseline niet vermeld, zes partijen).\par

In het onderzoek van Høyland et al. werd een classificatiemodel voor partij-affiliatie op basis van teksten getraind op het vijfde Europese Parlement (1999-2004) en getest op het zesde Europese Parlement (2004-2009) \cite{W14-2516}. Alle teksten van een spreker zijn samengevoegd tot één document. 40\% van de sprekers in de test set zaten ook in de training set. Hier verkregen zij een \textit{macro} $F_1$ score van 0.464 (baseline van 0.097) en \textit{accuracy} van 0.551 (baseline van 0.410). Hun baseline is op basis van altijd classificeren als grootste partij, terwijl voor $F_1$ score de baseline hoger ligt als hiervoor gekozen wordt voor gokken gewogen bij grootte van een klasse. \par

\subsection{Classificatiemethoden}
\label{sec:Deelvraag1}
Diermeier et al. \cite{diermeier_godbout_yu_kaufmann_2012} gebruikten Support Vector Machines. Verder maakten ze gebruik van \textit{tf-idf} met een minimale woordfrequentie van 50 en een minimale documentfrequentie van 10 en \textit{Part-Of-Speech tagging}.\par
Yu et al. \cite{doi:10.1080/19331680802149608} maakten gebruik van Support Vector Machines en Naive Bayes, waarvan de varianten multinomial en Bernoulli. De features waren unigrams, met minimale woordfrequentie van drie en de top 50 meest voorkomende woorden weggelaten. Voor de wegingen van de features bij Support Vector Machines werd geëxperimenteerd met \textit{boolean}, \textit{tf-norm} en \textit{tf-idf}. Het beste resultaat was afhankelijk van welke kamer Voor het huis van afgevaardigden was het Support Vector Machines met als weging \textit{tf-idf} en voor de senaat Bernouilli Naive Bayes.

Hirst et al. maakten  gebruik van Support Vector Machines \cite{Hirst_textto}. Ze experimenteerden met verschillende vormen van pre-processing, inclusief stemmen en het verwijderen van woorden op basis van te hoge of te lage frequentie. Deze variaties maakten in hun onderzoek geen grote verschillen en uiteindelijk is gekozen voor het niet stemmen, het weglaten van woorden die in minder dan vijf documenten voorkomen en resultaten van zowel met als zonder de top 500 meest frequente woorden. Daarnaast werd geëxperimenteerd met vier wegingen voor woorden: \textit{boolean}, \textit{tf}, \textit{tf-norm} en \textit{tf-idf}, waarvan \textit{tf-idf} het beste resultaat opleverde. \par
Bhand et al. gebruikten verschillende n-grams, inclusief verschillende manieren van \textit{smoothing}\cite{bhand}. Ze testten als weging voor features zowel \textit{boolean} als \textit{tf}, waarbij ze vonden concludeerden dat \textit{boolean} betere resultaten opleverden. Voor classificatiemodel experimenteerden ze met SVM en Naive Bayes . Voor het selecteren van \textit{features} experimenteerden ze met een minimale frequentie en selectie van woorden op basis van hoogste mutual information. Uiteindelijk was het beste model bij hen een SVM met uni- en bigrams en geselecteerd op basis van mutual information.\par
In het onderzoek van Ferreira werd gebruik gemaakt van twee classificatiemethoden: Logistische regressie en MIRA\cite{Ferreira2016UsingTT}. Logistische regressie werd aangevuld met \textit{group Lasso} regularisatie. Voor wegingen van woorden werd geëxperimenteerd met \textit{tf}, \textit{tf-idf}, \textit{$\Delta$-tf-idf} en \textit{$\Delta$-BM-25}. Daarnaast wordt er gebruik gemaakt van woordclustering, \textit{Concise Semantic Analysis} en stylometrische eigenschappen. Op \textit{Part-Of-Speech tagging} na hadden stylometrische eigenschappen een duidelijke negatieve invloed op de classificatie.\par
Høyland et al. maakten gebruik van Support Vector Machine\cite{W14-2516}. Als beste waarde voor de regularisatieterm, de C-parameter, vonden zij 0.8. Daarnaast gebruikten zij \textit{dependency disambiguated
stems} wat bij hen een $F_1$ score van twee procent hoger opleverden dan normale stemming.\par

\subsection{Invloed van partijnamen of sprekersnamen}
Diermeier et al. lieten de namen van de sprekers en verwijzingen naar staten die de senatoren representeren weg, omdat deze volgens hen de classificatie te makkelijk zouden maken \cite{diermeier_godbout_yu_kaufmann_2012}. Hirst et al. vinden inderdaad dat partijnamen (en het weglaten daarvan) bij het Europees Parlement een grote invloed hebben op de classificatie \cite{Hirst_textto}. Bij het Europees Parlement zien zij met name het gebruik van de eigen partijnaam door een spreker, terwijl zij in het Canadese parlement vooral zien dat de naam van de andere partij gebruikt wordt door een spreker.

\subsection{Invloed van oppositie of regering}
Hirst et al. vonden in hun onderzoek dat de classificatie van spreker in het Canadese parlement op basis van partij-affiliatie meer zegt over de status van de partij (regering of oppositie).\cite{Hirst_textto} Zo vergeleken zij de top tien karakteristieke woorden van de liberalen en conservatieven in het 36e parlement (liberalen in de regering) en het 39e parlement (conservatieven in de regering. Hier vonden zij dat vier van de tien woorden van de liberalen (regering) in het 36e parlement bij het 39e parlement bij de conservatieven (regering) te vinden waren. Andersom gebeurde hetzelfde met één van de tien woorden van de conservatieven (oppositie) in het 36e parlement naar liberalen (oppositie) in het 39e parlement.\par
In hetzelfde onderzoek trainden ze ook hun classificaties op het ene parlement en testten deze op het andere parlement. Hierbij vonden zij in beide gevallen een \textit{accuracy} ver onder de baseline. Daarnaast deden ze ook nog een classificatie op de sprekers die in beide parlementen zaten en een andere classificatie op sprekers die niet in beide parlementen zaten. Bij de eerste classificatie vonden ze \textit{accuracy} scores rond de baseline, terwijl in de tweede situatie \textit{accuracy} scores gevonden werden ver boven de baseline. \par
Deze resultaten leidden de onderzoekers tot de conclusie dat de classificatie voornamelijk het gevolg is van de status van de partij en minder van ideologie.\par



% https://nlp.stanford.edu/courses/cs224n/2009/fp/7.pdf