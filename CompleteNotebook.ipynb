{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xlrd\n",
    "import operator\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import openpyxl\n",
    "from pactools.grid_search import GridSearchCVProgressBar\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, product\n",
    "from decimal import Decimal\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "def importdata(file):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df = df.loc[df['speech category'] == 'Main Speech']\n",
    "    return df \n",
    "df = importdata(\"HandelingenRutte2.csv\")\n",
    "df['stemmed_text'] = df.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "df['normal_text'] = df.text.apply(lambda x: ' '.join([t.lower() for t in tokenizer.tokenize(x)]))\n",
    "df.drop(columns=['tags'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaving out all that is outside 95%\n",
    "list_text = df.stemmed_text.apply(lambda x: x.split(\" \"))\n",
    "t = [len(doc) for doc in list_text]\n",
    "print(np.mean(t),np.median(t))\n",
    "fit =scipy.stats.lognorm.fit(t)\n",
    "ax = sns.distplot(t,bins=1000,norm_hist=False,kde=False)\n",
    "ax.set_xscale('log')\n",
    "ax.set(xlabel='Woorden per document', ylabel='frequentie')\n",
    "plt.savefig(\"Verslag/Tables/lengthtextslog.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(t, bins=1000, norm_hist=False,kde=False)\n",
    "ax.set(xlabel='Woorden per document', ylabel='frequentie')\n",
    "plt.savefig(\"Verslag/Tables/lengthtexts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.stemmed_text.apply(lambda x: len(x.split(' '))) < math.exp(fit[0]+2*fit[1])]\n",
    "df = df.loc[df.stemmed_text.apply(lambda x: len(x.split(' '))) > math.exp(fit[0]-2*fit[1])]\n",
    "list_text = df.stemmed_text.apply(lambda x: x.split(\" \"))\n",
    "print(\"Ondergrens:\",math.exp(fit[0]+2*fit[1]),\"\\nBovengrens:\",math.exp(fit[0]-2*fit[1]))\n",
    "print(\"Mediaan\",np.median([len(doc) for doc in list_text]),\"\\nGemiddelde\",np.mean([len(doc) for doc in list_text]))\n",
    "print(\"Overgebleven documenten:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.file.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/11350770/pandas-dataframe-select-by-partial-string\n",
    "\n",
    "# Creates a dataframe with counts of dataset per party\n",
    "countdf = pd.concat([df.party.value_counts(),df[df['titel'].str.contains(\"Vragen van het lid\")].party.value_counts(), df[~df['titel'].str.contains(\"Vragen van het lid\")].party.value_counts()], axis=1)\n",
    "countdf.columns = ['Totaal', 'Vragenuur','Debat']\n",
    "countdf = countdf.sort_values(\"Totaal\",axis=0, ascending=False)\n",
    "with open(\"Verslag/Tables/Spreekbeurten.tex\", \"w\") as f:\n",
    "    f.write(countdf.to_latex(header=True))\n",
    "countdf = countdf.sort_index()\n",
    "countdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst = chain.from_iterable([list(set(doc)) for doc in list_text])\n",
    "del list_text\n",
    "count = Counter(tekst)\n",
    "samples = list(count.values())\n",
    "t = Counter(samples)\n",
    "t = sorted(t.items())\n",
    "x, y = zip(*t)\n",
    "plt.loglog(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleprediction = np.random.choice(countdf.index, len(df.party), p = [x/countdf[\"Totaal\"].sum() for x in list(countdf['Totaal'])])\n",
    "print(classification_report(df.party, simpleprediction))\n",
    "print(\"Accuracy:\",accuracy_score(df.party, np.full(len(df.party), countdf[\"Totaal\"].idxmax())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelsdownload(file,only_doen=False):\n",
    "    models_df = pd.read_excel(file, index_col=0)\n",
    "    if only_doen:\n",
    "        models_df = models_df.loc[models_df.DOEN == True]\n",
    "    models_dict = {}\n",
    "    for x,y in zip(models_df.Classifier, models_df.PIPELINE):\n",
    "        exec(compile(\"a=\"+y,'','exec'), globals())\n",
    "        models_dict[x] = Pipeline(a)\n",
    "    return models_df, models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df, models_dict = modelsdownload('Models.xlsx',True)\n",
    "\n",
    "# Creates a dict with parameters per classifier   \n",
    "params_dict = {}\n",
    "for clf in set(models_df.Classifier):\n",
    "    params_df = models_df.loc[models_df.Classifier == clf]\n",
    "    params_df = params_df.loc[params_df.DOEN == True]\n",
    "    params_df = params_df.dropna(axis=1, how='all')\n",
    "    params_df = params_df.drop([col for col in params_df.columns if \"__\" not in col], axis=1)\n",
    "    temp_dict = {}\n",
    "    for x, y in params_df.to_dict(orient='list').items():\n",
    "        a = literal_eval(y[0])\n",
    "        temp_dict[x] = a   \n",
    "    params_dict[clf] = temp_dict\n",
    "    \n",
    "text_dict = {x:y for x, y in zip(models_df.Classifier, models_df.STEMMED)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted','recall_weighted']\n",
    "for name, pipe in models_dict.items():\n",
    "    print(\"Bezig: %s\" %name)\n",
    "    if text_dict[name]:\n",
    "         text = df.stemmed_text\n",
    "    else:\n",
    "        text = df.normal_text\n",
    "        \n",
    "    gs = GridSearchCVProgressBar(pipe, params_dict[name], cv=5, scoring=scoring, refit = False, return_train_score = False, n_jobs=-1)\n",
    "    gs.fit(text, df.party)\n",
    "    rows = pd.DataFrame(gs.cv_results_)\n",
    "    # https://stackoverflow.com/questions/38231591/splitting-dictionary-list-inside-a-pandas-column-into-separate-columns\n",
    "    rows = pd.concat([rows.drop(['params'], axis=1), rows['params'].apply(pd.Series)], axis=1)\n",
    "    rows[\"name\"] = name \n",
    "    rows[\"stemmed\"] = text_dict[name]\n",
    "    pd.concat([rows, pd.read_excel('Scores.xlsx', index_col=0)], ignore_index=True).sort_values(['mean_test_f1_weighted'], ascending=False).to_excel('Scores.xlsx')\n",
    "    df4 = pd.read_excel('Models.xlsx', index_col=0)\n",
    "    df4.loc[df4.Classifier == name, \"DOEN\"] = False\n",
    "    df4.to_excel('Models.xlsx')\n",
    "    print(\"Voltooid: %s\" %name)\n",
    "    \n",
    "# https://stackoverflow.com/questions/46735847/save-best-params-in-gridsearch-in-a-pandas-dataframe\n",
    "# https://stackoverflow.com/questions/36271413/pandas-merge-nearly-duplicate-rows-based-on-column-value?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_excel('Scores.xlsx', index_col=0)\n",
    "ax = sns.kdeplot(scores[\"mean_test_accuracy\"], scores[\"mean_test_f1_weighted\"], shade=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('Echte klasse')\n",
    "    plt.xlabel('Voorspelde klasse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format\n",
    "def classifaction_report_csv(report,save):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    del lines[-3]\n",
    "    for line in lines[2:-1]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['Partij'] = row_data[-5]\n",
    "        row['Precision'] = float(row_data[-4])\n",
    "        row['Recall'] = float(row_data[-3])\n",
    "        row['F1_score'] = float(row_data[-2])\n",
    "        row['Documenten'] = int(row_data[-1])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe = dataframe.reindex([\"Partij\",\"Precision\",\"Recall\",\"F1_score\",\"Documenten\"], axis=1)\n",
    "\n",
    "    return dataframe.set_index('Partij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the classifier and parameters with highest score\n",
    "def getbestpipe(file):\n",
    "    scores = pd.read_excel(file, index_col=0).sort_values(['mean_test_f1_weighted'], ascending=False).head(1)\n",
    "    scores = scores.dropna(axis=1).reset_index(drop=True)\n",
    "    scores.vect__ngram_range[0] = literal_eval(scores.vect__ngram_range[0])\n",
    "    models_df, models_dict = modelsdownload('Models.xlsx',False)\n",
    "    text_dict = {x:y for x, y in zip(models_df.Classifier, models_df.STEMMED)}\n",
    "    pipe = models_dict[scores.name[0]]\n",
    "    params = scores.drop([col for col in scores.columns if \"__\" not in col or \"param\" in col], axis=1).to_dict(orient='records')[0]\n",
    "    params['vect__min_df'] = int(params['vect__min_df'])\n",
    "    pipe.set_params(**params)\n",
    "    stem = text_dict[scores.name[0]]\n",
    "    return pipe, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintest3(file, data, save=False):\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    pipe, stem = getbestpipe(file)\n",
    "    if stem:\n",
    "        pipe.fit(train.stemmed_text, train.party)\n",
    "        predicted = pipe.predict(test.stemmed_text)\n",
    "    else:\n",
    "        pipe.fit(train.normal_text,train.party)\n",
    "        predicted = pipe.predict(test.normal_text)\n",
    "    report = classifaction_report_csv(classification_report(test.party, predicted),False)\n",
    "    accuracy = accuracy_score(test.party, predicted)\n",
    "    cnf_matrix = confusion_matrix(test.party, predicted)\n",
    "    return pipe, cnf_matrix, report, accuracy\n",
    "    \n",
    "\n",
    "def traintest(file, data, save=False):\n",
    "    pipe, cnf_matrix, report, accuracy = traintest3(file, data, save=False)\n",
    "    for x in tqdm(range(4)):\n",
    "        _, cnf_matrix2, report2, accuracy2 = traintest3(file, data, save=False)\n",
    "        cnf_matrix = np.add(cnf_matrix,cnf_matrix2)\n",
    "        report = report.add(report2, fill_value=0)\n",
    "        accuracy += accuracy2\n",
    "    accuracy /=5\n",
    "    cnf_matrix = np.divide(cnf_matrix,5)\n",
    "    report /=5\n",
    "     \n",
    "    print(report)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plot_confusion_matrix(cnf_matrix.astype(int), \\\n",
    "                      classes=sorted(list(data.party.unique())), \\\n",
    "                      title='Confusion matrix best estimator')\n",
    "    if save:\n",
    "        plt.savefig(\"Verslag/Tables/%s.png\" %save)\n",
    "        with open(\"Verslag/Tables/%s.tex\" %save, \"w\") as f:\n",
    "            f.write(report.to_latex())\n",
    "\n",
    "    plt.show()\n",
    "    return pipe\n",
    "\n",
    "pipe = traintest('Scores.xlsx', df,save=\"confusionmatrix\")-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostimportantwords(pipe, countdf, filename):\n",
    "    vocab = pipe.named_steps['vect'].vocabulary_\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    coefs = pipe.named_steps[[x for x in pipe.named_steps.keys() if not x=='vect'][0]].coef_\n",
    "    top_words = [[inv_vocab[t] for t in coefs[i].argsort()[-10:][::-1]] for i in range(coefs.shape[0])]\n",
    "    top_words = pd.DataFrame(top_words).T\n",
    "    top_words.columns = countdf.index\n",
    "    with open(\"Verslag/Tables/%s1.tex\" %filename, \"w\") as f:\n",
    "        f.write(top_words.iloc[:,:5].to_latex(index=False))\n",
    "    with open(\"Verslag/Tables/%s2.tex\" %filename, \"w\") as f:\n",
    "        f.write(top_words.iloc[:,5:].to_latex(index=False))\n",
    "    return top_words\n",
    "\n",
    "mostimportantwords(pipe, countdf, 'MostImportantWords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Party_synomyns = [\"Partij van de Arbeid\",\"Socialistische Partij\",\"Partij voor de Dieren\",\"Volkspartij voor Vrijheid en Democratie\",\"Partij voor de Vrijheid\"]\n",
    "partijnamen = list(df.party.unique()) + Party_synomyns\n",
    "partijnamen += [\"[Hh]et \"+ x for x in partijnamen] + [\"[Dd]e \"+ x for x in partijnamen]\n",
    "partijnamen = re.compile('|'.join(partijnamen))\n",
    "kamerlidnamen = re.compile('|'.join(df.surname.unique()) + \"|Dijkstra|Van Dijk|Van Nieuwenhuizen|Baay|Berndsen|Jansen|Günal|Schut|Nijkerken|Buma|Venrooy|Mulder|De Vries|Berckmoes|Van Dijck|Pia\")\n",
    "temp = df.text.apply(lambda x: re.sub(kamerlidnamen,\"xxxxx\",re.sub(partijnamen,\"yyyyy\",x)).lower())\n",
    "df['stemmed_text'] = temp.apply(lambda x: ' '.join([stemmer.stem(t).replace(\"xxxxx\",'KAMERlIDNAAM').replace(\"yyyyy\", 'PARTIJNAAM') for t in tokenizer.tokenize(x)]))\n",
    "df['normal_text'] = temp.apply(lambda x: ' '.join([t.replace(\"xxxxx\",'KAMERlIDNAAM').replace(\"yyyyy\", 'PARTIJNAAM') for t in tokenizer.tokenize(x)]))\n",
    "del temp\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.xlsx', df, save=\"withoutnames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostimportantwords(pipe, countdf, 'MostImportantWordsWithoutNames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintest2(file, data, save=False):\n",
    "    train, test = data\n",
    "    pipe, stem = getbestpipe(file)\n",
    "    if stem:\n",
    "        pipe.fit(train.stemmed_text, train.party)\n",
    "        predicted = pipe.predict(test.stemmed_text)\n",
    "    else:\n",
    "        pipe.fit(train.normal_text,train.party)\n",
    "        predicted = pipe.predict(test.normal_text)\n",
    "    cm = confusion_matrix(test.party, predicted)\n",
    "    tot = len(predicted)\n",
    "    ggg = defaultdict(int)\n",
    "    mix = []\n",
    "    regering = []\n",
    "    oppositie = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[0]):\n",
    "            if i!=j:\n",
    "                a = (sum(cm[i,:])-cm[i,i]) * (sum(cm[j,:])/(tot-sum(cm[i,:])))\n",
    "                ggg[(i,sum(cm[i,:]))]+=a\n",
    "                if i in [6,10] and j in [6,10]:\n",
    "                    regering.append(cm[i,j]-a)\n",
    "                elif i in [0,1,2,3,4,5,7,8,9] and j in [0,1,2,3,4,5,7,8,9]:\n",
    "                    oppositie.append(cm[i,j]-a)\n",
    "                else:\n",
    "                    mix.append(cm[i,j]-a)\n",
    "\n",
    "    return pipe, oppositie, regering, mix, ggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = []\n",
    "m = []\n",
    "r = []\n",
    "g = {}\n",
    "for i in tqdm(range(10)):\n",
    "    pipe, oppositie, regering, mix, ggg=traintest2('Scores.xlsx', datasplitter(df, 0.8))\n",
    "    g ={**g, **ggg}\n",
    "    o.append(oppositie)\n",
    "    r.append(regering)\n",
    "    m.append(mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(keys,values) = zip(*g.items())\n",
    "party, keys = zip(*keys)\n",
    "keys = pd.Series(keys,name='fn + tp')\n",
    "values = pd.Series(values,name='fp')\n",
    "party = pd.Series(party,name=\"Partij\").replace(dict(enumerate(sorted(df.party.unique()))))\n",
    "q = sns.lmplot(x='fn + tp', y='fp', hue='Partij', data=pd.concat([keys, values, party], axis=1),fit_reg=False, palette=sns.color_palette(\"hls\", 11))\n",
    "ax = sns.regplot(keys, values, scatter=False, ax=q.axes[0, 0])\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Verslag/Tables/Correlation.png') \n",
    "scipy.stats.pearsonr(keys,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit gebruiken\n",
    "ax = sns.distplot(list(chain.from_iterable(o)),kde=False)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Verslag/Tables/Oppositie.png') \n",
    "print(\"Mean:\",np.mean(list(chain.from_iterable(o))))\n",
    "print(scipy.stats.normaltest(list(chain.from_iterable(o))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(list(chain.from_iterable(r)),kde=False)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Verslag/Tables/Regering.png') \n",
    "np.mean(list(chain.from_iterable(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(list(chain.from_iterable(m)),kde=False)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Verslag/Tables/Mix.png') \n",
    "np.mean(list(chain.from_iterable(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(list(chain.from_iterable(m+o+r)),kde=False)\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Verslag/Tables/Totaal.png')\n",
    "print(np.mean(list(chain.from_iterable(m+o+r))))\n",
    "print(scipy.stats.normaltest(list(chain.from_iterable(m+o+r))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.normaltest(list(chain.from_iterable(m+o+r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regering & Oppositie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PartijStatus'] = df['party'].map({'CDA': 'Oppositie', 'ChristenUnie': 'Oppositie', 'D66':'Oppositie', 'SGP':'Oppositie', 'VVD':'Regering', 'SP':'Oppositie','PvdD':'Oppositie','GroenLinks':'Oppositie','PvdA':'Regering','50PLUS':'Oppositie'})\n",
    "pipe = traintest('Scores.xlsx', datasplitter(df.dropna(), 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regering\n",
    "vocab = pipe.named_steps['vect'].vocabulary_\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "coefs = pipe.named_steps[[x for x in pipe.named_steps.keys() if not x=='vect'][0]].coef_\n",
    "top_words = []\n",
    "for i in range(coefs.shape[0]):\n",
    "    temp_list = []\n",
    "    for t in coefs[i].argsort()[-10:][::-1]:\n",
    "        temp_list.append(inv_vocab[t])\n",
    "    top_words.append(temp_list)\n",
    "top_words = pd.DataFrame(top_words).T\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oppositie\n",
    "vocab = pipe.named_steps['vect'].vocabulary_\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "coefs = pipe.named_steps[[x for x in pipe.named_steps.keys() if not x=='vect'][0]].coef_\n",
    "top_words = []\n",
    "for i in range(coefs.shape[0]):\n",
    "    temp_list = []\n",
    "    for t in coefs[i].argsort()[:10][::-1]:\n",
    "        temp_list.append(inv_vocab[t])\n",
    "    top_words.append(temp_list)\n",
    "top_words = pd.DataFrame(top_words).T\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Woorden Balkenende 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "dfb4 = importdata(\"HandelingenBalkenende4.csv\")\n",
    "dfb4['stemmed_text'] = dfb4.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "dfb4 = dfb4.loc[dfb4.stemmed_text.apply(lambda x: len(x.split(' '))) < math.exp(fit[0]+2*fit[1])]\n",
    "dfb4 = dfb4.loc[dfb4.stemmed_text.apply(lambda x: len(x.split(' '))) > math.exp(fit[0]-2*fit[1])]\n",
    "dfb4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe with counts of dataset per party\n",
    "countdfb4 = dfb4.party.value_counts().sort_index()\n",
    "countdfb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Party_synomyns = [\"Partij van de Arbeid\",\"Socialistische Partij\",\"Partij voor de Dieren\",\"Volkspartij voor Vrijheid en Democratie\",\"Partij voor de Vrijheid\"]\n",
    "partijnamen = list(dfb4.party.unique()) + Party_synomyns\n",
    "partijnamen += [\"[Hh]et \"+ x for x in partijnamen] + [\"[Dd]e \"+ x for x in partijnamen]\n",
    "partijnamen = re.compile('|'.join(partijnamen))\n",
    "kamerlidnamen = re.compile('|'.join(dfb4.surname.unique())+\"|Kortenhorst|Wiegman|De Roos|Schreijer|Willemse|Snijder|Van Vroonhoven|Dezentjé Hamming|Blanksma|Buma|De Pater|Ortega|Madsen\")\n",
    "dfb4['text'] = dfb4.text.apply(lambda x: re.sub(kamerlidnamen,\"xxxxx\",re.sub(partijnamen,\"yyyyy\",x)).lower().replace(\"xxxxx\",'KAMERlIDNAAM').replace(\"yyyyy\", 'PARTIJNAAM'))\n",
    "dfb4['stemmed_text'] = dfb4.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "dfb4['normal_text'] = dfb4.text.apply(lambda x: ' '.join([t for t in tokenizer.tokenize(x)]))\n",
    "dfb4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.xlsx', datasplitter(dfb4, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostimportantwords(pipe, countdfb4, 'MostImportantWordsWithoutNames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergelijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.xlsx', (df[df.party != '50PLUS'],dfb4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.xlsx', (dfb4,df[df.party != '50PLUS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links / Rechts vergelijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Kieskompas'] = df['party'].map({'CDA': 'Rechts', 'ChristenUnie': 'Rechts', 'D66':'Rechts', 'SGP':'Rechts', 'VVD':'Rechts', 'SP':'Links','PvdD':'Links','GroenLinks':'Links','PvdA':'Links','50PLUS':'Links'})\n",
    "pipe, predicted = traintest('Scores.xlsx', datasplitter(df.dropna(), 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, _ = getbestpipe('Scores.xlsx')\n",
    "df_train = df.dropna().sample(frac=0.8)\n",
    "df_test_index = list(set(df.index.tolist()) - set(df_train.index.tolist()))\n",
    "df_test = df.dropna()[df.dropna().index.isin(df_test_index)]\n",
    "pipe.fit(df_train.withoutnames, df_train.Kieskompas)\n",
    "predicted = pipe.predict(df_test.withoutnames)\n",
    "print(classification_report(df_test.Kieskompas, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull\n",
    "!git add CompleteNotebook.ipynb\n",
    "!git add Scraper.ipynb\n",
    "!git add Scores.xlsx\n",
    "!git add Scriptie_Sprekers_TK.pdf\n",
    "!git add Verslag/Tables/Spreekbeurten.tex\n",
    "!git add Verslag/Tables/MostImportantWordsWithoutNames1.tex\n",
    "!git add Verslag/Tables/MostImportantWordsWithoutNames2.tex\n",
    "!git add Verslag/Tables/MostImportantWords1.tex\n",
    "!git add Verslag/Tables/Correlation.png\n",
    "!git add Verslag/Tables/confusionmatrix.tex\n",
    "!git add Verslag/Tables/MostImportantWords2.tex\n",
    "!git add Verslag/Tables/lengthtextslog.png\n",
    "!git add Verslag/Tables/lengthtexts.png\n",
    "!git add Verslag/Tables/Totaal.png\n",
    "!git add Verslag/Tables/Mix.png\n",
    "!git add Verslag/Tables/withoutnames.tex\n",
    "!git add Verslag/Tables/Regering.png\n",
    "!git add Verslag/Tables/Oppositie.png\n",
    "!git add Models.xlsx\n",
    "!git add Verslag/Tables/confusionmatrix.png\n",
    "!git commit -m Update\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
