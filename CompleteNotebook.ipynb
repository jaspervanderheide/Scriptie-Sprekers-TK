{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sn\n",
    "import xlrd\n",
    "import operator\n",
    "import math\n",
    "import openpyxl\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "from decimal import Decimal\n",
    "from IPython.display import display, HTML\n",
    "from ast import literal_eval\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv(\"HandelingenRutte2.csv\", index_col=0)\n",
    "df = df.loc[df['speech category'] == 'Main Speech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_text'] = df.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "df['normal_text'] = df.text.apply(lambda x: ' '.join([t.lower() for t in tokenizer.tokenize(x)]))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe with counts of dataset per party\n",
    "countdf = df.party.value_counts().sort_index()\n",
    "with open(\"Verslag/Tables/Spreekbeurten.tex\", \"w\") as f:\n",
    "    f.write(countdf.to_latex(header=False))\n",
    "countdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tekst = chain.from_iterable([list(set(doc)) for doc in df.list_text])\n",
    "# count = Counter(tekst)\n",
    "# samples = list(count.values())\n",
    "# t = Counter(samples)\n",
    "# t = sorted(t.items())\n",
    "# x, y = zip(*t)\n",
    "# plt.loglog(x, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = Counter([len(doc) for doc in df.list_text])\n",
    "# t = sorted(t.items())\n",
    "# x, y = zip(*t)\n",
    "# plt.plot(x, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelsdownload(only_doen=False):\n",
    "    models_df = pd.read_excel('Models.xlsx', index_col=0)\n",
    "    if only_doen:\n",
    "        models_df = models_df.loc[models_df.DOEN == True]\n",
    "    models_dict = {}\n",
    "    for x,y in zip(models_df.Classifier, models_df.PIPELINE):\n",
    "        exec(compile(\"a=\"+y,'','exec'), globals())\n",
    "        models_dict[x] = Pipeline(a)\n",
    "    return models_df, models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df, models_dict = modelsdownload(True)\n",
    "\n",
    "# Creates a dict with parameters per classifier   \n",
    "params_dict = {}\n",
    "for clf in set(models_df.Classifier):\n",
    "    params_df = models_df.loc[models_df.Classifier == clf]\n",
    "    params_df = params_df.loc[params_df.DOEN == True]\n",
    "    params_df = params_df.dropna(axis=1, how='all')\n",
    "    params_df = params_df.drop([col for col in params_df.columns if \"__\" not in col], axis=1)\n",
    "    temp_dict = {}\n",
    "    for x, y in params_df.to_dict(orient='list').items():\n",
    "        a = literal_eval(y[0])\n",
    "        temp_dict[x] = a   \n",
    "    params_dict[clf] = temp_dict\n",
    "    \n",
    "text_dict = {x:y for x, y in zip(models_df.Classifier, models_df.RAW)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted','recall_weighted']\n",
    "for name, pipe in models_dict.items():\n",
    "    \n",
    "    if text_dict[name]:\n",
    "         text = list(df.normal_text)\n",
    "    else:\n",
    "        text = list(df.stemmed_text)\n",
    "        \n",
    "    gs = GridSearchCV(pipe, params_dict[name], cv=5, scoring=scoring, refit = False, return_train_score = False, n_jobs=5)\n",
    "    gs.fit(text, df.party)\n",
    "    rows = pd.DataFrame(gs.cv_results_)\n",
    "    # https://stackoverflow.com/questions/38231591/splitting-dictionary-list-inside-a-pandas-column-into-separate-columns\n",
    "    rows = pd.concat([rows.drop(['params'], axis=1), rows['params'].apply(pd.Series)], axis=1)\n",
    "    rows[\"name\"] = name\n",
    "    pd.concat([rows, pd.read_csv('Scores.csv', index_col=0)], ignore_index=True).sort_values(['mean_test_f1_weighted'], ascending=False).to_csv('Scores.csv')\n",
    "    df4 = pd.read_excel('Models.xlsx', index_col=0)\n",
    "    df4.loc[df4.Classifier == name, \"DOEN\"] = False\n",
    "    df4.to_excel('Models.xlsx')\n",
    "    print(\"voltooid:%s\" %name)\n",
    "    \n",
    "# https://stackoverflow.com/questions/46735847/save-best-params-in-gridsearch-in-a-pandas-dataframe\n",
    "# https://stackoverflow.com/questions/36271413/pandas-merge-nearly-duplicate-rows-based-on-column-value?rq=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the classifier and parameters with highest score\n",
    "scores = pd.read_csv('Scores.csv', index_col=0).head(1)\n",
    "scores = scores.dropna(axis=1).reset_index(drop=True)\n",
    "scores.vect__ngram_range[0] = literal_eval(scores.vect__ngram_range[0])\n",
    "models_df, models_dict = modelsdownload(False)\n",
    "pipe = models_dict[scores.name[0]]\n",
    "pipe.set_params(**scores.drop([col for col in scores.columns if \"__\" not in col or \"param\" in col], axis=1).to_dict(orient='records')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8)\n",
    "df_test_index = list(set(df.index.tolist()) - set(df_train.index.tolist()))\n",
    "df_test = df[df.index.isin(df_test_index)]\n",
    "if text_dict[name]:\n",
    "    pipe.fit(list(df_train.normal_text), list(df_train.party))\n",
    "    predicted = pipe.predict(df_test.normal_text)\n",
    "else:\n",
    "    pipe.fit(list(df_train.stemmed_text), list(df_train.party))\n",
    "    predicted = pipe.predict(df_test.stemmed_text)\n",
    "print(classification_report(df_test.party, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pipe.named_steps['vect'].vocabulary_\n",
    "inv_vocab = {v: k for k, v in woord.items()}\n",
    "coefs = pipe.named_steps['SGD'].coef_\n",
    "top_words = []\n",
    "for i in range(coefs.shape[0]):\n",
    "    temp_list = []\n",
    "    for t in coefs[i].argsort()[-10:][::-1]:\n",
    "        temp_list.append(inv_vocab[t])\n",
    "    top_words.append(temp_list)\n",
    "top_words = pd.DataFrame(top_words).T\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "cnf_matrix = confusion_matrix(df_test.party, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, \\\n",
    "                      classes=sorted(list(df.party.unique())), \\\n",
    "                      title='Confusion matrix best estimator')\n",
    "plt.savefig(\"Verslag/confusionmatrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull\n",
    "!git add CompleteNotebook.ipynb\n",
    "!git add Scraper.ipynb\n",
    "!git add Scores.csv\n",
    "!git add Scriptie_Sprekers_TK.pdf\n",
    "!git add Verslag/Tables/Spreekbeurten.tex\n",
    "#!git add Verslag/Tables/MItable.tex\n",
    "#!git add Verslag/Tables/MItable2.tex\n",
    "!git add Models.xlsx\n",
    "!git add Verslag/confusionmatrix.png\n",
    "!git commit -m Update\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
