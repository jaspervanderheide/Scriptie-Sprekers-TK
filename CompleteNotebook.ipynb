{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import xlrd\n",
    "import operator\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "import scipy\n",
    "import openpyxl\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from decimal import Decimal\n",
    "from IPython.display import display, HTML\n",
    "from ast import literal_eval\n",
    "from pactools.grid_search import GridSearchCVProgressBar\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv(\"HandelingenRutte2.csv\", index_col=0)\n",
    "df = df.loc[df['speech category'] == 'Main Speech']\n",
    "df['stemmed_text'] = df.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "df['normal_text'] = df.text.apply(lambda x: ' '.join([t.lower() for t in tokenizer.tokenize(x)]))\n",
    "# Uitschieters weghalen.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaving out all that is outside 95%\n",
    "list_text = df.stemmed_text.apply(lambda x: x.split(\" \"))\n",
    "t = [len(doc) for doc in list_text]\n",
    "print(np.mean(t),np.median(t))\n",
    "fit =scipy.stats.lognorm.fit(t)\n",
    "ax = sns.distplot(t,bins=1000,norm_hist=False,kde=False)\n",
    "ax.set_xscale('log')\n",
    "plt.savefig(\"Verslag/Tables/lengthtextslog.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(t, bins=1000, norm_hist=False,kde=False)\n",
    "plt.savefig(\"Verslag/Tables/lengthtexts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.stemmed_text.apply(lambda x: len(x.split(' '))) < math.exp(fit[0]+2*fit[1])]\n",
    "df = df.loc[df.stemmed_text.apply(lambda x: len(x.split(' '))) > math.exp(fit[0]-2*fit[1])]\n",
    "print(math.exp(fit[0]+2*fit[1]),math.exp(fit[0]-2*fit[1]))\n",
    "list_text = df.stemmed_text.apply(lambda x: x.split(\" \"))\n",
    "print(\"mediaan\",np.median([len(doc) for doc in list_text]),\"gemiddelde\",np.mean([len(doc) for doc in list_text]))\n",
    "print(\"overgebleven documenten:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.file.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/11350770/pandas-dataframe-select-by-partial-string\n",
    "# Creates a dataframe with counts of dataset per party\n",
    "countdf = pd.concat([df.party.value_counts(),df[df['titel'].str.contains(\"Vragen van het lid\")].party.value_counts(), df[~df['titel'].str.contains(\"Vragen van het lid\")].party.value_counts()], axis=1)\n",
    "countdf.columns = ['Totaal', 'Vragenuur','Debat']\n",
    "countdf = countdf.sort_values(\"Totaal\",axis=0, ascending=False)\n",
    "with open(\"Verslag/Tables/Spreekbeurten.tex\", \"w\") as f:\n",
    "    f.write(countdf.to_latex(header=True))\n",
    "countdf = countdf.sort_index()\n",
    "countdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst = chain.from_iterable([list(set(doc)) for doc in list_text])\n",
    "count = Counter(tekst)\n",
    "samples = list(count.values())\n",
    "t = Counter(samples)\n",
    "t = sorted(t.items())\n",
    "x, y = zip(*t)\n",
    "plt.loglog(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleprediction = np.random.choice(countdf.index, len(df.party), p = [x/countdf[\"Totaal\"].sum() for x in list(countdf['Totaal'])])\n",
    "print(classification_report(df.party, simpleprediction))\n",
    "print(\"Accuracy:\",accuracy_score(df.party, np.full(len(df.party), countdf[\"Totaal\"].idxmax())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelsdownload(only_doen=False):\n",
    "    models_df = pd.read_excel('Models.xlsx', index_col=0)\n",
    "    if only_doen:\n",
    "        models_df = models_df.loc[models_df.DOEN == True]\n",
    "    models_dict = {}\n",
    "    for x,y in zip(models_df.Classifier, models_df.PIPELINE):\n",
    "        exec(compile(\"a=\"+y,'','exec'), globals())\n",
    "        models_dict[x] = Pipeline(a)\n",
    "    return models_df, models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df, models_dict = modelsdownload(True)\n",
    "\n",
    "# Creates a dict with parameters per classifier   \n",
    "params_dict = {}\n",
    "for clf in set(models_df.Classifier):\n",
    "    params_df = models_df.loc[models_df.Classifier == clf]\n",
    "    params_df = params_df.loc[params_df.DOEN == True]\n",
    "    params_df = params_df.dropna(axis=1, how='all')\n",
    "    params_df = params_df.drop([col for col in params_df.columns if \"__\" not in col], axis=1)\n",
    "    temp_dict = {}\n",
    "    for x, y in params_df.to_dict(orient='list').items():\n",
    "        a = literal_eval(y[0])\n",
    "        temp_dict[x] = a   \n",
    "    params_dict[clf] = temp_dict\n",
    "    \n",
    "text_dict = {x:y for x, y in zip(models_df.Classifier, models_df.STEMMED)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'f1_weighted', 'precision_weighted','recall_weighted']\n",
    "for name, pipe in models_dict.items():\n",
    "    \n",
    "    if text_dict[name]:\n",
    "         text = list(df.normal_text)\n",
    "    else:\n",
    "        text = list(df.stemmed_text)\n",
    "        \n",
    "    gs = GridSearchCVProgressBar(pipe, params_dict[name], cv=5, scoring=scoring, refit = False, return_train_score = False, n_jobs=5)\n",
    "    gs.fit(text, df.party)\n",
    "    rows = pd.DataFrame(gs.cv_results_)\n",
    "    # https://stackoverflow.com/questions/38231591/splitting-dictionary-list-inside-a-pandas-column-into-separate-columns\n",
    "    rows = pd.concat([rows.drop(['params'], axis=1), rows['params'].apply(pd.Series)], axis=1)\n",
    "    rows[\"name\"] = name\n",
    "    rows[\"stemmed\"] = text_dict[name]\n",
    "    pd.concat([rows, pd.read_csv('Scores.csv', index_col=0)], ignore_index=True).sort_values(['mean_test_f1_weighted'], ascending=False).to_csv('Scores.csv')\n",
    "    df4 = pd.read_excel('Models.xlsx', index_col=0)\n",
    "    df4.loc[df4.Classifier == name, \"DOEN\"] = False\n",
    "    df4.to_excel('Models.xlsx')\n",
    "    print(\"Voltooid: %s\" %name)\n",
    "    \n",
    "# https://stackoverflow.com/questions/46735847/save-best-params-in-gridsearch-in-a-pandas-dataframe\n",
    "# https://stackoverflow.com/questions/36271413/pandas-merge-nearly-duplicate-rows-based-on-column-value?rq=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('Echte klasse')\n",
    "    plt.xlabel('Voorspelde klasse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the classifier and parameters with highest score\n",
    "def getbestpipe(file):\n",
    "    scores = pd.read_csv(file, index_col=0).head(1)\n",
    "    scores = scores.dropna(axis=1).reset_index(drop=True)\n",
    "    scores.vect__ngram_range[0] = literal_eval(scores.vect__ngram_range[0])\n",
    "    models_df, models_dict = modelsdownload(False)\n",
    "    text_dict = {x:y for x, y in zip(models_df.Classifier, models_df.STEMMED)}\n",
    "    pipe = models_dict[scores.name[0]]\n",
    "    params = scores.drop([col for col in scores.columns if \"__\" not in col or \"param\" in col], axis=1).to_dict(orient='records')[0]\n",
    "    params['vect__min_df'] = int(params['vect__min_df'])\n",
    "    pipe.set_params(**params)\n",
    "    stem = text_dict[scores.name[0]]\n",
    "    return pipe, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintest(file, data, save=False):\n",
    "    train, test = data\n",
    "    pipe, stem = getbestpipe(file)\n",
    "    if stem:\n",
    "        pipe.fit(train.stemmed_text, train.party)\n",
    "        predicted = pipe.predict(test.stemmed_text)\n",
    "    else:\n",
    "        pipe.fit(train.normal_text,train.party)\n",
    "        predicted = pipe.predict(test.normal_text)\n",
    "    print(classification_report(test.party, predicted))\n",
    "    print('Accuracy: ', accuracy_score(test.party, predicted))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    cnf_matrix = confusion_matrix(test.party, predicted)\n",
    "    plot_confusion_matrix(cnf_matrix, \\\n",
    "                      classes=sorted(list(test.party.unique())), \\\n",
    "                      title='Confusion matrix best estimator')\n",
    "    if save:\n",
    "        plt.savefig(\"Verslag/Tables/%s.png\" %save)\n",
    "    plt.show()\n",
    "    return pipe\n",
    "\n",
    "def datasplitter(dataframe, frac):\n",
    "    df_train = dataframe.sample(frac=frac)\n",
    "    df_test_index = list(set(dataframe.index.tolist()) - set(df_train.index.tolist()))\n",
    "    df_test = dataframe[dataframe.index.isin(df_test_index)]\n",
    "    return df_train, df_test\n",
    "\n",
    "pipe = traintest('Scores.csv', datasplitter(df, 0.8),save=\"confusionmatrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostimportantwords(pipe, countdf, filename):\n",
    "    vocab = pipe.named_steps['vect'].vocabulary_\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    coefs = pipe.named_steps[[x for x in pipe.named_steps.keys() if not x=='vect'][0]].coef_\n",
    "    top_words = []\n",
    "    for i in range(coefs.shape[0]):\n",
    "        temp_list = []\n",
    "        for t in coefs[i].argsort()[-10:][::-1]:\n",
    "            temp_list.append(inv_vocab[t])\n",
    "        top_words.append(temp_list)\n",
    "    top_words = pd.DataFrame(top_words).T\n",
    "    top_words.columns = countdf.index\n",
    "    with open(\"Verslag/Tables/%s1.tex\" %filename, \"w\") as f:\n",
    "        f.write(top_words.iloc[:,:5].to_latex(index=False))\n",
    "    with open(\"Verslag/Tables/%s2.tex\" %filename, \"w\") as f:\n",
    "        f.write(top_words.iloc[:,5:].to_latex(index=False))\n",
    "    return top_words\n",
    "\n",
    "mostimportantwords(pipe, countdf, 'MostImportantWords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Party_synomyns = [\"Partij van de Arbeid\",\"Socialistische Partij\",\"Partij voor de Dieren\",\"Volkspartij voor Vrijheid en Democratie\",\"Partij voor de Vrijheid\"]\n",
    "partijnamen = list(df.party.unique()) + Party_synomyns\n",
    "partijnamen += [\"[Hh]et \"+ x for x in partijnamen] + [\"[Dd]e \"+ x for x in partijnamen]\n",
    "partijnamen = re.compile('|'.join(partijnamen))\n",
    "kamerlidnamen = re.compile('|'.join(df.surname.unique()) + \"|Dijkstra|Van Dijk|Van Nieuwenhuizen|Baay|Berndsen|Jansen|Günal|Schut|Nijkerken|Buma|Venrooy|Mulder|De Vries|Berckmoes|Van Dijck|Pia\")\n",
    "df['text'] = df.text.apply(lambda x: re.sub(kamerlidnamen,\"xxxxx\",re.sub(partijnamen,\"yyyyy\",x)).lower().replace(\"xxxxx\",'KAMERlIDNAAM').replace(\"yyyyy\", 'PARTIJNAAM'))\n",
    "df['stemmed_text'] = df.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "df['normal_text'] = df.text.apply(lambda x: ' '.join([t for t in tokenizer.tokenize(x)]))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.csv', datasplitter(df, 0.8), save=\"confusionmatrix2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostimportantwords(pipe, countdf, 'MostImportantWordsWithoutNames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regering & Oppositie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PartijStatus'] = df['party'].map({'CDA': 'Oppositie', 'ChristenUnie': 'Oppositie', 'D66':'Oppositie', 'SGP':'Oppositie', 'VVD':'Regering', 'SP':'Oppositie','PvdD':'Oppositie','GroenLinks':'Oppositie','PvdA':'Regering','50PLUS':'Oppositie'})\n",
    "pipe = traintest('Scores.csv', datasplitter(df.dropna(), 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regering\n",
    "vocab = pipe.named_steps['vect'].vocabulary_\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "coefs = pipe.named_steps[[x for x in pipe.named_steps.keys() if not x=='vect'][0]].coef_\n",
    "top_words = []\n",
    "for i in range(coefs.shape[0]):\n",
    "    temp_list = []\n",
    "    for t in coefs[i].argsort()[-10:][::-1]:\n",
    "        temp_list.append(inv_vocab[t])\n",
    "    top_words.append(temp_list)\n",
    "top_words = pd.DataFrame(top_words).T\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oppositie\n",
    "vocab = pipe.named_steps['vect'].vocabulary_\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "coefs = pipe.named_steps[[x for x in pipe.named_steps.keys() if not x=='vect'][0]].coef_\n",
    "top_words = []\n",
    "for i in range(coefs.shape[0]):\n",
    "    temp_list = []\n",
    "    for t in coefs[i].argsort()[:10][::-1]:\n",
    "        temp_list.append(inv_vocab[t])\n",
    "    top_words.append(temp_list)\n",
    "top_words = pd.DataFrame(top_words).T\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Woorden Balkenende 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "dfb4 = pd.read_csv(\"HandelingenBalkenende4.csv\", index_col=0)\n",
    "dfb4 = dfb4.loc[dfb4['speech category'] == 'Main Speech']\n",
    "dfb4['stemmed_text'] = dfb4.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "dfb4 = dfb4.loc[dfb4.stemmed_text.apply(lambda x: len(x.split(' '))) < math.exp(fit[0]+2*fit[1])]\n",
    "dfb4 = dfb4.loc[dfb4.stemmed_text.apply(lambda x: len(x.split(' '))) > math.exp(fit[0]-2*fit[1])]\n",
    "dfb4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe with counts of dataset per party\n",
    "countdfb4 = dfb4.party.value_counts().sort_index()\n",
    "countdfb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Party_synomyns = [\"Partij van de Arbeid\",\"Socialistische Partij\",\"Partij voor de Dieren\",\"Volkspartij voor Vrijheid en Democratie\",\"Partij voor de Vrijheid\"]\n",
    "partijnamen = list(dfb4.party.unique()) + Party_synomyns\n",
    "partijnamen += [\"[Hh]et \"+ x for x in partijnamen] + [\"[Dd]e \"+ x for x in partijnamen]\n",
    "partijnamen = re.compile('|'.join(partijnamen))\n",
    "kamerlidnamen = re.compile('|'.join(dfb4.surname.unique())+\"|Kortenhorst|Wiegman|De Roos|Schreijer|Willemse|Snijder|Van Vroonhoven|Dezentjé Hamming|Blanksma|Buma|De Pater|Ortega|Madsen\")\n",
    "dfb4['text'] = dfb4.text.apply(lambda x: re.sub(kamerlidnamen,\"xxxxx\",re.sub(partijnamen,\"yyyyy\",x)).lower().replace(\"xxxxx\",'KAMERlIDNAAM').replace(\"yyyyy\", 'PARTIJNAAM'))\n",
    "dfb4['stemmed_text'] = dfb4.text.apply(lambda x: ' '.join([stemmer.stem(t) for t in tokenizer.tokenize(x)]))\n",
    "dfb4['normal_text'] = dfb4.text.apply(lambda x: ' '.join([t for t in tokenizer.tokenize(x)]))\n",
    "dfb4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.csv', datasplitter(dfb4, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostimportantwords(pipe, countdfb4, 'MostImportantWordsWithoutNames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergelijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.csv', (df[df.party != '50PLUS'],dfb4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = traintest('Scores.csv', (dfb4,df[df.party != '50PLUS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links / Rechts vergelijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Kieskompas'] = df['party'].map({'CDA': 'Rechts', 'ChristenUnie': 'Rechts', 'D66':'Rechts', 'SGP':'Rechts', 'VVD':'Rechts', 'SP':'Links','PvdD':'Links','GroenLinks':'Links','PvdA':'Links','50PLUS':'Links'})\n",
    "pipe, predicted = traintest('Scores.csv', datasplitter(df.dropna(), 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, _ = getbestpipe('Scores.csv')\n",
    "df_train = df.dropna().sample(frac=0.8)\n",
    "df_test_index = list(set(df.index.tolist()) - set(df_train.index.tolist()))\n",
    "df_test = df.dropna()[df.dropna().index.isin(df_test_index)]\n",
    "pipe.fit(df_train.withoutnames, df_train.Kieskompas)\n",
    "predicted = pipe.predict(df_test.withoutnames)\n",
    "print(classification_report(df_test.Kieskompas, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull\n",
    "!git add CompleteNotebook.ipynb\n",
    "!git add Scraper.ipynb\n",
    "!git add Scores.csv\n",
    "!git add Scriptie_Sprekers_TK.pdf\n",
    "!git add Verslag/Tables/Spreekbeurten.tex\n",
    "!git add Verslag/Tables/MostImportantWordsWithoutNames1.tex\n",
    "!git add Verslag/Tables/MostImportantWordsWithoutNames2.tex\n",
    "!git add Verslag/Tables/MostImportantWords1.tex\n",
    "!git add Verslag/Tables/MostImportantWords2.tex\n",
    "!git add Verslag/Tables/lengthtextslog.png\n",
    "!git add Verslag/Tables/lengthtexts.png\n",
    "!git add Models.xlsx\n",
    "!git add Verslag/Tables/confusionmatrix.png\n",
    "!git commit -m Update\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
